{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2453786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sparsemax import Sparsemax\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DotProductScore(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotProductScore, self).__init__()\n",
    "        # 定义可学习的参数 q，使用 nn.Parameter 将其注册为模型参数\n",
    "        # q 是一个二维张量，形状为 (hidden_size, 1)，表示注意力分数的权重\n",
    "        self.q = nn.Parameter(torch.empty(size=(hidden_size, 1), dtype=torch.float32))\n",
    "        # 初始化权重\n",
    "        self.init_weights()\n",
    "        self.sparsemax = Sparsemax(dim=1)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # 初始化权重的范围\n",
    "        initrange = 0.5\n",
    "        # 用均匀分布填充参数 q 的数据\n",
    "        self.q.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            - X：输入矩阵，inputs=[batch_size,seq_length,hidden_size]\n",
    "        输出：\n",
    "            - scores：输出矩阵，shape=[batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        # 计算注意力分数，使用点积注意力\n",
    "        scores = torch.matmul(inputs, self.q)\n",
    "        scores = self.sparsemax(scores)\n",
    "        # 压缩张量的最后一个维度，将其从 (batch_size, seq_length, 1) 变为 (batch_size, seq_length)\n",
    "        scores = scores.squeeze(-1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scores = DotProductScore(hidden_size)\n",
    "        self.sparsemax = Sparsemax(dim=1)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        scores = self.scores(X)\n",
    "        arrange = torch.arange(X.size(1), dtype=torch.float32, device=X.device).unsqueeze(0)\n",
    "        mask = (arrange < valid_lens.unsqueeze(-1)).float()\n",
    "        scores = scores * mask - (1 - mask) * 1e9\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)  # 保留 Softmax 用于计算注意力权重\n",
    "        attention_weights = self.sparsemax(attention_weights) \n",
    "        out = torch.matmul(attention_weights.unsqueeze(1), X).squeeze(1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ModelLSTMAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, ins_num):\n",
    "        super(ModelLSTMAttention, self).__init__()\n",
    "        self.ins_num = ins_num\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        # 添加 Batch Normalization 层\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size * 2)  # 使用 hidden_size * 2，根据你的模型要求进行调整\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc_1 = nn.Linear(self.ins_num, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, seq, valid_lens):\n",
    "        \n",
    "        output, _ = self.lstm(seq)\n",
    "        valid_lens = valid_lens.view(-1,).to(device)\n",
    "        out = self.attention(output, valid_lens)\n",
    "        \n",
    "        # 应用 Batch Normalization\n",
    "        out = self.bn1(out) \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.reshape(-1, self.ins_num)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 读取 TSV 文件并提取指定列的信息\n",
    "def read_tsv(filename, inf_ind, skip_1st=False, file_encoding=\"utf8\"):\n",
    "    extract_inf = []\n",
    "    with open(filename, \"r\", encoding=file_encoding) as tsv_f:\n",
    "        if skip_1st:\n",
    "            tsv_f.readline()\n",
    "        line = tsv_f.readline()\n",
    "        while line:\n",
    "            line_list = line.strip().split(\"\\t\")\n",
    "            temp_inf = [line_list[ind] for ind in inf_ind]\n",
    "            extract_inf.append(temp_inf)\n",
    "            line = tsv_f.readline()\n",
    "    return extract_inf\n",
    "\n",
    "# 读取氨基酸特征文件并生成特征字典\n",
    "def get_features(filename, f_num=15):\n",
    "    f_list = read_tsv(filename, list(range(16)), True)\n",
    "    f_dict = {}\n",
    "    left_num = 0\n",
    "    right_num = 0\n",
    "    if f_num > 15:\n",
    "        left_num = (f_num - 15) // 2\n",
    "        right_num = f_num - 15 - left_num\n",
    "    for f in f_list:\n",
    "        f_dict[f[0]] = [0] * left_num + [float(x) for x in f[1:]] + [0] * right_num\n",
    "    f_dict[\"X\"] = [0] * f_num\n",
    "    return f_dict\n",
    "\n",
    "def generate_input(sps, sp_lbs, feature_dict, feature_num, ins_num, max_len):\n",
    "    # 用于存储输入数据、标签和序列长度的列表\n",
    "    xs, ys, lens = [], [], []\n",
    "\n",
    "    # 遍历每个样本（sp表示样本）\n",
    "    for i, sp in enumerate(sps):\n",
    "        # 添加样本的标签\n",
    "        ys.append(sp_lbs[i])\n",
    "\n",
    "        # 将每条序列的原始长度添加到列表中，空序列用0填充\n",
    "        lens.extend([len(tcr[0]) if tcr[0] else 0 for tcr in sp])\n",
    "\n",
    "    # 确保序列数量为 ins_num\n",
    "    while len(lens) % ins_num != 0:\n",
    "        lens = np.concatenate((lens, np.array([0])))  # 添加一个空序列\n",
    "\n",
    "    # 将 lens 转换为 NumPy 数组，为了后续的维度调整\n",
    "    lens = np.array(lens)\n",
    "\n",
    "    # 将 lens 调整为正确的形状\n",
    "    lens = lens.reshape(-1, ins_num)\n",
    "\n",
    "    # 检查是否有缺失的样本，如果有则进行填充\n",
    "    while lens.shape[0] < len(sps):\n",
    "        lens = np.concatenate((lens, np.zeros((1, ins_num))), axis=0)\n",
    "\n",
    "    # 遍历每个样本（sp表示样本）\n",
    "    for i, sp in enumerate(sps):\n",
    "        # 初始化一个3D张量，用于存储特征矩阵，初始值全为0\n",
    "        # 使用列表推导式来确保为每个样本创建一个新的列表\n",
    "        x = [[[0] * feature_num for _ in range(max_len)] for _ in range(ins_num)]\n",
    "\n",
    "        # 遍历样本中的每条序列（tcr表示T细胞受体序列）\n",
    "        seq_count = 0  # 用于计数实际插入的序列数量\n",
    "        for j, tcr in enumerate(sp):\n",
    "            # 获取序列的氨基酸序列\n",
    "            tcr_seq = tcr[0]\n",
    "            # 计算需要填充的右侧数量，以便使序列达到指定的最大长度\n",
    "            right_num = max_len - len(tcr_seq)\n",
    "\n",
    "            # 在氨基酸序列右侧填充'X'，使其达到最大长度\n",
    "            tcr_seq += \"X\" * right_num\n",
    "\n",
    "            # 用于存储氨基酸特征矩阵\n",
    "            tcr_matrix = []\n",
    "\n",
    "            # 遍历氨基酸序列中的每个氨基酸，将其特征添加到矩阵中\n",
    "            for aa in tcr_seq:\n",
    "                tcr_matrix.append(feature_dict[aa.upper()])\n",
    "\n",
    "            # 将填充后的特征矩阵放入3D张量中的相应位置\n",
    "            x[seq_count] = tcr_matrix\n",
    "            seq_count += 1\n",
    "\n",
    "        xs.append(x)\n",
    "\n",
    "    # 将列表转换为NumPy数组\n",
    "    xs = np.array(xs)\n",
    "\n",
    "    # 转换为PyTorch张量，指定数据类型为float32\n",
    "    xs = torch.tensor(xs, dtype=torch.float32)\n",
    "\n",
    "    # 交换张量的维度，将最后两个维度互换\n",
    "    xs = xs.swapaxes(2, 3)\n",
    "\n",
    "    # 将样本标签转换为NumPy数组\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # 将标签转换为PyTorch张量，指定数据类型为float32，并调整维度\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # 将序列长度转换为PyTorch张量\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "\n",
    "    # 返回生成的输入数据、标签和序列长度\n",
    "\n",
    "    return xs, ys, lens\n",
    "\n",
    "def load_data(sample_dir):\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for sample_file in os.listdir(sample_dir):\n",
    "        # 读取样本数据\n",
    "        training_data.append(read_tsv(os.path.join(sample_dir, sample_file), [0, 1], True))\n",
    "        # 获取样本标签\n",
    "        if \"P\" in sample_file:\n",
    "            training_labels.append(1)\n",
    "        elif \"H\" in sample_file:\n",
    "            training_labels.append(0)\n",
    "        else:\n",
    "            print(\"Wrong sample filename! Please name positive samples with 'P' and negative samples with 'H'.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    return training_data, training_labels\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def evaluate(model, criterion, test_loader, device='cpu'):\n",
    "    test_total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_batch_x, test_batch_y, test_valid_lens in test_loader:\n",
    "            test_batch_x = test_batch_x.view(-1, 24, 15).to(device)\n",
    "            test_batch_y = test_batch_y.to(device)\n",
    "            test_pred = model(test_batch_x, test_valid_lens)\n",
    "\n",
    "            test_loss = criterion(test_pred, test_batch_y)\n",
    "            test_total_loss += test_loss.item()\n",
    "            all_preds.append(test_pred.cpu().numpy())\n",
    "            all_labels.append(test_batch_y.cpu().numpy())\n",
    "            \n",
    "        test_avg_loss = test_total_loss / len(test_loader)\n",
    "        return test_avg_loss, all_preds, all_labels\n",
    "\n",
    "def train(fold, model, criterion, optimizer, train_loader, test_loader, epoches=100, device='cpu'):\n",
    "    \n",
    "    model_path = f'../model(LSTMY)/{disease_name}checkpoint{fold}.pt'  # 修改模型文件的保存路径和命名\n",
    "    early_stopping = EarlyStopping(PATIENCE, path=model_path, verbose=False)\n",
    "    \n",
    "    # 存储训练和测试损失\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    with tqdm(total=epoches) as t:\n",
    "        t.set_description(f'{disease_name} - Fold {fold}')  # 添加疾病名称\n",
    "        for epoch in range(epoches):\n",
    "            lr = adjust_learning_rate(epoch)  # 调整学习率，调用adjust_learning_rate函数，返回当前轮次的学习率\n",
    "            if cur_lr != lr: # 如果当前学习率不等于调整后的学习率\n",
    "                cur_lr = lr # 将当前学习率更新为调整后的学习率\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=cur_lr) # 更新优化器，将模型参数和更新后的学习率传入\n",
    "            \n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_x, batch_y, valid_lens in train_loader:\n",
    "                batch_x = batch_x.view(-1, 24, 15).to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                pred = model(batch_x, valid_lens)\n",
    "\n",
    "                loss = criterion(pred, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # 记录训练损失\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_train_losses.append(avg_loss)\n",
    "            # 记录评估损失\n",
    "            test_avg_loss, _, _ = evaluate(model, criterion, test_loader, device=device)\n",
    "            epoch_test_losses.append(test_avg_loss)\n",
    "            \n",
    "            t.set_postfix(loss=avg_loss, test_loss=test_avg_loss)\n",
    "            t.update(1)\n",
    "            \n",
    "            # 向EarlyStopping类别添加新轮次的损失\n",
    "            early_stopping(test_avg_loss, model)\n",
    "            # 判别是否满足提前退出的条件\n",
    "            if early_stopping.early_stop:\n",
    "                # 恢复训练中的最优模型\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                #print('Early stopping')\n",
    "                break\n",
    "def metrics(all_preds, all_labels, threshold=0.5):\n",
    "    # 计算二进制分类指标\n",
    "    binary_preds = (all_preds > threshold).astype(int)\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1787a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on RA dataset: 322 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RA - Fold 0:   0%|                                                                             | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cur_lr' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m\n\u001b[0;32m     83\u001b[0m cur_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m \u001b[38;5;66;03m# 初始学习率\u001b[39;00m\n\u001b[0;32m     84\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcur_lr) \u001b[38;5;66;03m# 优化器，使用Adam优化器，将模型参数和学习率传入\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# 在测试集上进行最终评估\u001b[39;00m\n\u001b[0;32m     90\u001b[0m _, preds, labels \u001b[38;5;241m=\u001b[39m evaluate(model, criterion, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[3], line 255\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(fold, model, criterion, optimizer, train_loader, test_loader, epoches, device)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoches):\n\u001b[0;32m    254\u001b[0m     lr \u001b[38;5;241m=\u001b[39m adjust_learning_rate(epoch)  \u001b[38;5;66;03m# 调整学习率，调用adjust_learning_rate函数，返回当前轮次的学习率\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcur_lr\u001b[49m \u001b[38;5;241m!=\u001b[39m lr: \u001b[38;5;66;03m# 如果当前学习率不等于调整后的学习率\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         cur_lr \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;66;03m# 将当前学习率更新为调整后的学习率\u001b[39;00m\n\u001b[0;32m    257\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcur_lr) \u001b[38;5;66;03m# 更新优化器，将模型参数和更新后的学习率传入\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'cur_lr' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# 学习率调度\n",
    "def adjust_learning_rate(epoch):\n",
    "    if epoch < 100:\n",
    "      return 0.0001\n",
    "    elif epoch < 200:\n",
    "      return 0.00005\n",
    "    else:\n",
    "      return 0.00001\n",
    "#参数设置\n",
    "def init_model():\n",
    "    input_size = 15\n",
    "    hidden_size =20\n",
    "    num_layers = 3\n",
    "    output_size = 1\n",
    "    ins_num = 100\n",
    "    dropout = 0.5\n",
    "    \n",
    "    return ModelLSTMAttention(input_size, hidden_size, output_size, num_layers, dropout, ins_num)\n",
    "\n",
    "# 引入早停机制\n",
    "sys.path.append('../')\n",
    "from python_codes.pytorchtools import EarlyStopping\n",
    "\n",
    "# 读取氨基酸特征文件\n",
    "aa_file = \"../Data/PCA15.txt\"\n",
    "aa_vectors = get_features(aa_file)  # 请确保 get_features 函数正确读取文件\n",
    "\n",
    "# 5折交叉验证\n",
    "k_fold = 5\n",
    "kf = KFold(n_splits=k_fold, shuffle=True,random_state=42)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHES = 500\n",
    "PATIENCE = 100\n",
    "\n",
    "all_accuracies = []  # 存储每一折的准确率\n",
    "all_sensitivities = []  # 存储每一折的灵敏度\n",
    "all_specificities = []  # 存储每一折的特异度\n",
    "all_aucs = []  # 存储每一折的AUC值\n",
    "\n",
    "device = \"cuda\"\n",
    "disease_list = [\"RA\", \"T1D\", \"MS\", \"IAA\"]\n",
    "results = []\n",
    "results_ROC = []\n",
    "\n",
    "for disease_name in disease_list:\n",
    "    data_dir = f'../Data/{disease_name}'\n",
    "    training_data, training_labels = load_data(data_dir)\n",
    "    print(f\"Working on {disease_name} dataset: {len(training_data)} samples\")\n",
    "    \n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    \n",
    "    # 分成5折\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(training_data)):\n",
    "        train_data = [training_data[i] for i in train_idx]\n",
    "        train_labels = [training_labels[i] for i in train_idx]\n",
    "        test_data = [training_data[i] for i in test_idx]\n",
    "        test_labels = [training_labels[i] for i in test_idx]\n",
    "        \n",
    "        \n",
    "        # 训练集和测试集固定后，再将训练集划分为训练集和验证集\n",
    "        train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=1234)\n",
    "\n",
    "        \n",
    "        train_input_batch, train_label_batch, train_valid_lens_batch = generate_input(train_data, train_labels, aa_vectors, 15, 100, 24)\n",
    "        valid_input_batch, valid_label_batch, valid_valid_lens_batch = generate_input(valid_data, valid_labels, aa_vectors, 15, 100, 24)\n",
    "        test_input_batch, test_label_batch, test_valid_lens_batch = generate_input(test_data, test_labels, aa_vectors, 15, 100, 24)\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(train_input_batch, train_label_batch, train_valid_lens_batch)\n",
    "        valid_dataset = Data.TensorDataset(valid_input_batch, valid_label_batch, valid_valid_lens_batch)\n",
    "        test_dataset = Data.TensorDataset(test_input_batch, test_label_batch, test_valid_lens_batch)\n",
    "\n",
    "        train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = Data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = init_model().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        cur_lr = 0.0001 # 初始学习率\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=cur_lr) # 优化器，使用Adam优化器，将模型参数和学习率传入\n",
    "\n",
    "\n",
    "        train(fold, model, criterion, optimizer, train_loader, valid_loader, epoches=NUM_EPOCHES, device=device)\n",
    "\n",
    "        # 在测试集上进行最终评估\n",
    "        _, preds, labels = evaluate(model, criterion, test_loader, device=device)\n",
    "        all_preds += preds\n",
    "        all_labels += labels\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    accuracy, sensitivity, specificity, auc = metrics(all_preds, all_labels)\n",
    "    print(f\"Mean Accuracy ({disease_name}): {accuracy:.4f}\")\n",
    "    print(f\"Mean Sensitivity ({disease_name}): {sensitivity:.4f}\")\n",
    "    print(f\"Mean Specificity ({disease_name}): {specificity:.4f}\")\n",
    "    print(f\"Mean AUC ({disease_name}): {auc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'disease': disease_name,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'auc': auc\n",
    "    })\n",
    "\n",
    "    results_ROC.append({\n",
    "        'disease': disease_name,\n",
    "        'auc': auc,\n",
    "        'all_preds': all_preds,\n",
    "        'all_labels': all_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2373b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
