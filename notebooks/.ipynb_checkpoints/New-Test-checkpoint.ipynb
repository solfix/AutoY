{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79590971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sparsemax import Sparsemax\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define functions to read TSV files\n",
    "def read_tsv(filename, inf_ind, skip_1st=False, file_encoding=\"utf8\"):\n",
    "    extract_inf = []\n",
    "    with open(filename, \"r\", encoding=file_encoding) as tsv_f:\n",
    "        if skip_1st:\n",
    "            tsv_f.readline()\n",
    "        line = tsv_f.readline()\n",
    "        while line:\n",
    "            line_list = line.strip().split(\"\\t\")\n",
    "            temp_inf = []\n",
    "            for ind in inf_ind:\n",
    "                temp_inf.append(line_list[ind])\n",
    "            extract_inf.append(temp_inf)\n",
    "            line = tsv_f.readline()\n",
    "    return extract_inf\n",
    "\n",
    "# Define a function that reads an amino acid feature file and generates a feature dictionary\n",
    "def get_features(filename, f_num=15):\n",
    "    f_list = read_tsv(filename, list(range(16)), True)\n",
    "    f_dict = {}\n",
    "    left_num = 0\n",
    "    right_num = 0\n",
    "    if f_num > 15:\n",
    "        left_num = (f_num - 15) // 2\n",
    "        right_num = f_num - 15 - left_num\n",
    "    for f in f_list:\n",
    "        f_dict[f[0]] = [0] * left_num\n",
    "        f_dict[f[0]] += [float(x) for x in f[1:]]\n",
    "        f_dict[f[0]] += [0] * right_num\n",
    "    f_dict[\"X\"] = [0] * f_num\n",
    "    return f_dict\n",
    "\n",
    "# Defining Model Classes\n",
    "class AotoY(nn.Module):\n",
    "    def __init__(self, ins_num, drop_out, n=[12, 12, 12, 6], k=[2, 3, 4, 5]):\n",
    "        super(AotoY, self).__init__()\n",
    "\n",
    "        self.ins_num = ins_num\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=15, out_channels=n[0], kernel_size=k[0], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=n[0], out_channels=n[0], kernel_size=k[0], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=15, out_channels=n[1], kernel_size=k[1], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=n[1], out_channels=n[1], kernel_size=k[1], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=15, out_channels=n[2], kernel_size=k[2], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=n[2], out_channels=n[2], kernel_size=k[2], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=15, out_channels=n[3], kernel_size=k[3], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=n[3], out_channels=n[3], kernel_size=k[3], stride=1, padding=0),\n",
    "            nn.BatchNorm1d(n[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "        self.fc_1 = nn.Linear(sum(n), 1)\n",
    "        self.dropout = nn.Dropout(p=drop_out) \n",
    "        self.fc_2 = nn.Linear(self.ins_num, 2)\n",
    "        self.dropout = nn.Dropout(p=drop_out) \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 15, 24)\n",
    "        \n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(x)\n",
    "        out3 = self.conv3(x)\n",
    "        out4 = self.conv4(x)\n",
    "        out = torch.cat([out1, out2, out3, out4], dim=1)\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = self.fc_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(-1, self.ins_num)\n",
    "        out = self.fc_2(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Defining Input Functions\n",
    "def generate_input(sps, sp_lbs, feature_dict, feature_num, ins_num, max_len):\n",
    "    xs, ys = [], []\n",
    "    i = 0\n",
    "    for sp in sps:\n",
    "        xs.append([[[0] * feature_num] * max_len] * ins_num)\n",
    "        ys.append(sp_lbs[i])\n",
    "        j = 0\n",
    "        for tcr in sp:\n",
    "            tcr_seq = tcr[0]\n",
    "            right_num = max_len - len(tcr_seq)\n",
    "            tcr_seq += \"X\" * right_num\n",
    "            tcr_matrix = []\n",
    "            for aa in tcr_seq:\n",
    "                tcr_matrix.append(feature_dict[aa.upper()])\n",
    "            xs[i][j] = tcr_matrix\n",
    "            j += 1\n",
    "        i += 1\n",
    "    xs = np.array(xs)\n",
    "    xs = xs.swapaxes(2, 3)\n",
    "    ys = np.array(ys)\n",
    "    return xs, ys\n",
    "\n",
    "#Define the Generate Label function\n",
    "def load_data(sample_dir):\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for sample_file in os.listdir(sample_dir):\n",
    "        training_data.append(read_tsv(os.path.join(sample_dir, sample_file), [0, 1], True))\n",
    "        if \"P\" in sample_file:\n",
    "            training_labels.append(1)\n",
    "        elif \"H\" in sample_file:\n",
    "            training_labels.append(0)\n",
    "        else:\n",
    "            print(\"Wrong sample filename! Please name positive samples with 'P' and negative samples with 'H'.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    return training_data, training_labels\n",
    "\n",
    "#Define the evaluation function\n",
    "def evaluate(model, criterion, test_loader, device=\"cuda\"):\n",
    "    test_total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_batch_x, test_batch_y in test_loader:\n",
    "            test_batch_x = test_batch_x.to(device)\n",
    "            test_batch_y = test_batch_y.to(device)\n",
    "            test_pred = model(test_batch_x)\n",
    "\n",
    "            test_loss = criterion(test_pred, test_batch_y)\n",
    "            test_total_loss += test_loss.item()\n",
    "            all_preds.append(test_pred.cpu().numpy())\n",
    "            all_labels.append(test_batch_y.cpu().numpy())\n",
    "            \n",
    "        test_avg_loss = test_total_loss / len(test_loader)\n",
    "        return test_avg_loss, all_preds, all_labels\n",
    "    \n",
    "#Define the training function    \n",
    "from tqdm import tqdm\n",
    "def train(fold, model, criterion, optimizer, train_loader, valid_loader, epoches=100, device='cuda'):\n",
    "    \n",
    "    \n",
    "    model_path = f'../model(MS)/{disease_name}checkpoint{fold}.pt'  # Save path of the model file\n",
    "    early_stopping = EarlyStopping(PATIENCE, path=model_path, verbose=False)\n",
    "    \n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    with tqdm(total=epoches) as t:\n",
    "        t.set_description(f'{disease_name} - Fold {fold}')  \n",
    "        for epoch in range(epoches):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                pred = model(batch_x)\n",
    "\n",
    "                loss = criterion(pred, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "           \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_train_losses.append(avg_loss)\n",
    "            test_avg_loss, _, _ = evaluate(model, criterion, test_loader, device=device)\n",
    "            epoch_test_losses.append(test_avg_loss) \n",
    "            t.set_postfix(loss=avg_loss, test_loss=test_avg_loss)\n",
    "            t.update(1)\n",
    "            early_stopping(test_avg_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                #print('Early stopping')\n",
    "                break\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))                \n",
    "\n",
    "# Define a function to compute a binary classification indicator                \n",
    "def metrics(all_preds, all_labels, threshold=0.5):\n",
    "    all_probs = sigmoid(all_preds[:, 1] - all_preds[:, 0])\n",
    "    binary_preds = (all_probs > threshold).astype(int)\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    return accuracy, sensitivity, specificity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5a0161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solfi\\AppData\\Local\\Temp\\ipykernel_18756\\2452830480.py:241: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(new_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 计算评估指标\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m accuracy, sensitivity, specificity, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew MS Samples - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sensitivity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msensitivity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Specificity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecificity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 242\u001b[0m, in \u001b[0;36mmetrics\u001b[1;34m(all_preds, all_labels, threshold)\u001b[0m\n\u001b[0;32m    240\u001b[0m sensitivity \u001b[38;5;241m=\u001b[39m conf_matrix[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m (conf_matrix[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m conf_matrix[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    241\u001b[0m specificity \u001b[38;5;241m=\u001b[39m conf_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m (conf_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m conf_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 242\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, sensitivity, specificity, auc\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "# Model parameterization\n",
    "def init_model():\n",
    "    ins_num = 100\n",
    "    drop_out = 0.5\n",
    "    return AotoY(ins_num, drop_out)\n",
    "\n",
    "# Introduce an early stop mechanism\n",
    "sys.path.append('../')\n",
    "from python_codes.pytorchtools import EarlyStopping\n",
    "\n",
    "# Reading amino acid profile files\n",
    "aa_file = \"../Data1/PCA15.txt\"\n",
    "aa_vectors = get_features(aa_file) \n",
    "\n",
    "device=\"cuda\"\n",
    "# 假设模型文件名为 'MS_model.pt'，并存储在 '../model(AutoY)/' 文件夹中\n",
    "model_path = '../model(MS)/MScheckpoint0.pt'\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = init_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# 处理新的MS样本\n",
    "new_data_dir = '../New-MS/'  # 新样本的文件夹路径\n",
    "new_data, new_labels = load_data(new_data_dir)  # 假设新样本也有标签\n",
    "\n",
    "# 将新样本转换为模型可接受的格式\n",
    "new_input_batch, new_label_batch = generate_input(new_data, new_labels, aa_vectors, 15, 100, 24)\n",
    "new_input_batch = torch.Tensor(new_input_batch).to(device)\n",
    "new_label_batch = torch.LongTensor(new_label_batch).to(device)\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "new_dataset = Data.TensorDataset(new_input_batch, new_label_batch)\n",
    "new_loader = Data.DataLoader(new_dataset, batch_size=len(new_input_batch), shuffle=False)\n",
    "\n",
    "# 使用模型进行预测\n",
    "_, new_preds, new_labels = evaluate(model, criterion, new_loader, device=device)\n",
    "new_preds = np.concatenate(new_preds, axis=0)\n",
    "new_labels = np.concatenate(new_labels, axis=0)\n",
    "\n",
    "# 计算评估指标\n",
    "accuracy, sensitivity, specificity, auc = metrics(new_preds, new_labels)\n",
    "print(f\"New MS Samples - Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5249a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
